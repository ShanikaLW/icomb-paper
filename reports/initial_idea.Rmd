---
title: "Hierarchical"
author: "Farshid"
date: "2023-04-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Two characteristics of forecast reconciliation

The main thrust of our research project is that hierarchical reconciliation of forecasts is beneficial for two reasons:

-   It imposes hierarchical additivity constraints on the forecasts. Since these constraints are valid constraints for the actual data, this is likely to improve forecasts.

-   It combines information from different forecasts. This is likely to move forecasts based on limited information set towards forecasts that are based on a larger information set, which is likely to improve forecasts.

We believe that the latter is not emphasised in the literature sufficiently, even though most simulation based evidence for the benefits of reconciliation is based on reconciling forecasts that are based on univariate information sets.

To emphasis the second characteristic, consider the $S$ matrix and its projection matrix that is used for reconciliation in the simplest hierarchy of one aggregate ($y$) and two base forecasts ($b1$ and $b2$):

```{r}
S <- matrix(c(1, 1, 0, 1, 0, 1), nrow = 3, ncol = 2)
projS <- S %*% solve(t(S) %*% S) %*% t(S)
print(projS)
```

We can see that this projection matrix combines the forecasts by placing 2/3 weight on the direct forecast and 1/3 weight on the indirect forecast:

$$ 
y^r = \frac{2}{3} y + \frac{1}{3} (b1+b2) \\
b1^r = \frac{2}{3} b1 + \frac{1}{3} (y-b2) \\
b2^r = \frac{2}{3} b2 + \frac{1}{3} (y-b1)
$$ 

This highlights the connection between forecast reconciliation and forecast combination, and suggests that there may be room for transferring insights from the forecast combination literature to the forecast reconciliation problem.

# Bias correction and optimal combination

If a history of forecasts are available, $MinT$ reconciliation uses a non-orthogonal projection that uses the history of forecast errors and provides a combination of forecasts whose weights take the accuracy of forecasts into account. 

Most of the theoretical properties of forecast reconciliation are based on the assumption of unbiased raw forecasts. However, unbiasedness is defined in a very strict sense that is unlikely to be satisfied in practice. Traditionally, unbiased forecasts are those whose unconditional mean of the forecast error is zero. However, unbiasedness in the hierarchical forecast reconciliation literature is defined as the conditional mean of the forecast errors given the entire history of all variables is zero. This means that all forecasts are optimal (in MSE sense) given the entire observed history. However, even in the simulation exercises in this literature, forecasts are generated from univariate time series models that only use the univariate information set, not the ensemble information set.

The research question is should one adhere to combining direct and indirect forecasts, or simply try to improve forecast by combining all available forecasts. In terms of the above example, for forecasting $b1$ should one look for $w*b1 + (1-w)*(y1-b2)$ or $\alpha_0\, y + \alpha_1\, b1 + \alpha_2 \, b2$? The latter emphasises the importance of combination of information sets more than the importance of hierarchical constraints.

## Proof of the concept

To prove the concept, we generate data from a model with a single common factor.

$$
f_t = 0.6 f_{t-1} + e_{1,t}\\
y_{2,t} = 1 + 0.8 f_t + e_{2,t} \\
y_{3,t} = 1 + 0.8 f_t + e_{3,t} \\
y_{1,t} = y_{2,t}+y_{3,t}
$$
This implies that $y_{1,t}$, $y_{2,t}$ and $y_{3,t}$ have univariate ARMA(1,1) representations. While the AR parameter in these representations will be 0.6 for all three series, determining the MA parameter requires solving specific quadratic equations in which the product of the two roots is 1, and choosing the root that is smaller than 1 in absolute value.

This function is to figure out the roots of a quadratic equation. This is to find out the implied univariate time series representation of time series generated by our bespoke DGP.

```{r}
quad <- function(a, b, c)
{
  a <- as.complex(a)
  answer <- c((-b + sqrt(b^2 - 4 * a * c)) / (2 * a),
              (-b - sqrt(b^2 - 4 * a * c)) / (2 * a))
  if(all(Im(answer) == 0)) answer <- Re(answer)
  if(answer[1] == answer[2]) return(answer[1])
  answer
}
```

For the two bottom level series:

```{r}
quad(a = 1, b = (10/3), c = 1)
```

And for the aggregate:

```{r}
quad(a = 1, b = 4.4, c = 1)
```

Let's find out the optimal minimum trace and information combination weights for our particular DGP the lazy way, i.e. from a very long time series:

```{r}
set.seed(3423457)
n <- 1000000
sigb1 <- 1
sigb2 <- 1
sigf <- 1
rho <- 0.6
e1 <- rnorm(n + 1)
e2 <- rnorm(n + 1)
ef <- rnorm(n + 1)
f <- rep(0, n + 1)
b1f <- rep(0, n + 1) # base forecast for the first bottom level
b2f <- rep(0, n + 1) # base forecast for the second bottom level
yf <- rep(0, n + 1) # base forecast for the aggregate

f[1] <- 1.25 * ef[1] # initial draw from the unconditional distribution
for (i in 2:(n + 1)) {
  f[i]<- 0.6 * f[i - 1] + ef[i]
}
b1 <- 1 + 0.8 * f + e1
b2 <- 1 + 0.8 * f + e2
y <- b1 + b2
Y <- cbind(y, b1, b2)
Yin <- Y[2:(n + 1), ] # estimation sample

# Conditioning on the first observation being known
b1f[1] <- b1[1] 
b2f[1] <- b2[1] 
yf[1] <- y[1] 

# Generating base forecasts from the implied univariate ARMA(1,1) models
for (i in 2:(n + 1)) {
  b1f[i] <- 1 + 0.6 * (b1[i - 1] - 1) - (b1[i - 1] - b1f[i - 1])/3
  b2f[i] <- 1 + 0.6 * (b2[i - 1] - 1) - (b2[i - 1] - b2f[i - 1])/3
  yf[i] <- 2 + 0.6 * (y[i - 1] - 2) - 0.2404082 * (y[i - 1] - yf[i - 1])
}
b2falt <- 1 + 0.8 * f # forecast of the second bottom level assuming knowledge of f_t
# base forecasts for the estimation and evaluation samples
Yfb <- cbind(yf, b1f, b2f) 
Yfbin <- Yfb[2:(n + 1), ]
Yfbalt <- cbind(yf, b1f, b2falt)
Yfbaltin <- Yfbalt[2:(n + 1), ]

E <- Yin - Yfbin # errors in the base forecast in the estimation sample
Ealt <- Yin - Yfbaltin

W <- var(E)
Walt <- var(Ealt)
invW <- solve(W)
invWalt <- solve(Walt)

S <- matrix(c(1, 1, 0, 1, 0, 1), nrow = 3, ncol = 2)

Bmint <- invW %*% S %*% solve(t(S) %*% invW %*% S) %*% t(S)

Bmintalt <- invWalt %*% S %*% solve(t(S) %*% invWalt %*% S) %*% t(S)

Bicomb <- solve(t(Yfbin) %*% Yfbin) %*% t(Yfbin) %*% Yin

Bicombalt <- solve(t(Yfbaltin) %*% Yfbaltin) %*% t(Yfbaltin) %*% Yin

Bols <- S %*% solve(t(S) %*% S) %*% t(S)
```

Let's look at these matrices one by one. The OLS one was given above. The OLS one is an orthogonal projection, so it is symmetric. However, the MinT matrix, while it is also a projection (all its eigenvalues are either 0 or 1), but because it is not an orthogonal projection, it is not symmetric. We present it in a way that if it is postmultiplied by the column vector of base forecasts, it produces a column vector of reconciled forecasts.

```{r}
t(Bmint)
```

It is interesting to see that it almost generates the aggregate by only using the aggregate, but for the bottom level series it uses a 50-50 combination of direct and indirect forecasts. Perhaps if one could figure these theoretically, the entries would have been 1, 0.5, -0.5 and 0.

The corresponding information combination matrix is:

```{r}
t(Bicomb)
```

Again, interestingly, and in hindsight intuitively, the information combination method uses the top level forecast only, and uses half of that for forecast of each bottom level series, i.e., a top down approach that is theoretically optimal for this DGP, if we had the optimal forecast of the top level. While the univariate ARMA forecast is not the optimal forecast, it seems that it is not too far off. Perhaps if one could find these on paper from theory, the entries would have been 1, 0.5, 0.5 in the first column and zeros elsewhere. At least there is strong reason to believe that the weights on the two bottom level series should be equal, even if they are not zero.

In the more interesting scenario that the forecaster of the second bottom level series is clever and has some information over and above historical time series information (i.e., they know $f_t$), the MinT matrix becomes:

```{r}
t(Bmintalt)
```

and the information combination matrix becomes:

```{r}
t(Bicombalt)
```

The information combination basically only uses the forecast of the second bottom level and uses that as the forecast of the first bottom level and 2 times that as the forecast of the top level, which makes perfect sense. MinT also takes the forecast of the second bottom level as is, and gives it a large weight in constructing the other forecasts, but I can't see any obvious interpretation for the magnitudes.

Now let's use these matrices in constructing one step ahead forecasts for 10000 independent observation. I generate these observations independently (rather than sequentially), and I burn in 100 observations in the hope of eliminating the effect of initial conditions in ARMA forecasts.

```{r}
nrep <- 10000   # number of replications
BASE <- matrix(nrow = nrep, ncol = 3)
BASEalt <- matrix(nrow = nrep, ncol = 3)
OLS <- matrix(nrow = nrep, ncol = 3)
OLSalt <- matrix(nrow = nrep, ncol = 3)
MINT <- matrix(nrow = nrep, ncol = 3)
MINTalt <- matrix(nrow = nrep, ncol = 3)
ICOMB <- matrix(nrow = nrep, ncol = 3)
ICOMBalt <- matrix(nrow = nrep, ncol = 3)
n <- 100       # just to remove the effect of initial conditions
h <- 1       # forecast horizon

for (irep in 1:nrep) {
  # All analysis is conditional on the first observation being known
  # So, generate one more data point than necessary
  e1 <- rnorm(n + h + 1)
  e2 <- rnorm(n + h + 1)
  ef <- rnorm(n + h + 1)
  f <- rep(0, n + h + 1)
  b1f <- rep(0, n + h + 1) # base forecast for the first bottom level
  b2f <- rep(0, n + h + 1) # base forecast for the second bottom level
  yf <- rep(0, n + h + 1) # base forecast for the aggregate
  
  f[1] <- 1.25 * ef[1] # initial draw from the unconditional distribution
  for (i in 2:(n + h + 1)) {
    f[i] <- 0.6 * f[i - 1] + ef[i]
  }
  b1 <- 1 + 0.8 * f + e1
  b2 <- 1 + 0.8 * f + e2
  y <- b1 + b2
  Y <- cbind(y, b1, b2)
  
  Yout <- Y[n + h + 1, ]  # evaluation observation
  
  # Conditioning on the first observation being known
  b1f[1] <- b1[1] 
  b2f[1] <- b2[1] 
  yf[1] <- y[1] 
  
  # Generating base forecasts from the implied univariate ARMA(1,1) models
  for (i in 2:(n + h + 1)) {
    b1f[i] <- 1 + 0.6 * (b1[i - 1] - 1) - (b1[i - 1] - b1f[i - 1])/3
    b2f[i] <- 1 + 0.6 * (b2[i - 1] - 1) - (b2[i - 1] - b2f[i - 1])/3
    yf[i] <- 2 + 0.6 * (y[i - 1] - 2) - 0.2404082 * (y[i - 1] - yf[i - 1])
  }
  b2falt <- 1 + 0.8 * f # forecast of the second bottom level assuming knowledge of f_t
  # base forecasts for evaluation
  Yfb <- cbind(yf, b1f, b2f) 
  Yfbout <- Yfb[n + h + 1, ] # evaluation forecast
  Yfbalt <- cbind(yf, b1f, b2falt)
  Yfbaltout <- Yfbalt[n + h + 1, ]
  
  mintf <- Yfbout %*% Bmint
  mintfalt <- Yfbaltout %*% Bmintalt
  
  icombf <- Yfbout %*% Bicomb
  icombfalt <- Yfbaltout %*% Bicombalt
  
  olsf <- Yfbout %*% Bols
  olsfalt <- Yfbaltout %*% Bols
  
  BASE[irep, ] <- Yout - Yfbout
  BASEalt[irep, ] <- Yout - Yfbaltout
  
  OLS[irep, ] <- Yout - olsf
  OLSalt[irep, ] <- Yout - olsfalt
  
  MINT[irep, ] <- Yout - mintf
  MINTalt[irep, ] <- Yout - mintfalt
  
  ICOMB[irep, ] <- Yout - icombf
  ICOMBalt[irep, ] <- Yout - icombfalt
}

baseMSE <- sum(BASE^2)/nrep
olsMSE <- sum(OLS^2)/nrep
mintMSE <- sum(MINT^2)/nrep
icombMSE <- sum(ICOMB^2)/nrep
MSE <- data.frame(baseMSE, olsMSE, mintMSE, icombMSE)

basealtMSE <- sum(BASEalt^2)/nrep
olsaltMSE <- sum(OLSalt^2)/nrep
mintaltMSE <- sum(MINTalt^2)/nrep
icombaltMSE <- sum(ICOMBalt^2)/nrep
altMSE <- data.frame(basealtMSE, olsaltMSE, mintaltMSE, icombaltMSE)
```

The trace of the MSE matrices for different forecasts when all three base forecasts are from the implied ARMA models are:

```{r}
print.data.frame(MSE)
```

And when the second bottom level has information about the common factor, the trace of the MSE matrices are:

```{r}
print.data.frame(altMSE)
```
